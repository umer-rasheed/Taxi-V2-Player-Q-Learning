{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Taxi-V2 Player via Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Modules\n"
     ]
    }
   ],
   "source": [
    "# IMPORT MODULES\n",
    "# Import Numpy, Gym etc\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "print('Import Modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Load Taxi-V2 Environment\n",
      "In this Environment the Yellow Square represents the Taxi, the (“|”) represents a Wall.\n",
      "The Blue Letter represents the Pick-Up Location, and the Purple letter is the Drop-Off Location.\n",
      "The Taxi will turn Green when it has a Passenger Aboard.\n",
      "The Environment gives a -1 Reward for each Step in order for the Agent to try and find the quickest solution.\n",
      "The Environment gives a -10 Reward if Agent incorrectly Picks Up or Drops Off a Passenger.\n",
      "The Environment gives a 20 Reward on Success\n"
     ]
    }
   ],
   "source": [
    "# CREATE ENVIRONMENT\n",
    "# Load Taxi-V2 Environment\n",
    "# In this Environment the Yellow Square represents the Taxi, the (“|”) represents a Wall, the Blue Letter represents the Pick-Up Location, and the Purple letter is the Drop-Off Location. The Taxi will turn Green when it has a Passenger Aboard. \n",
    "Env=gym.make(\"Taxi-v2\")\n",
    "Env.render()\n",
    "print('Load Taxi-V2 Environment') \n",
    "print('In this Environment the Yellow Square represents the Taxi, the (“|”) represents a Wall.')\n",
    "print('The Blue Letter represents the Pick-Up Location, and the Purple letter is the Drop-Off Location.')\n",
    "print('The Taxi will turn Green when it has a Passenger Aboard.')\n",
    "print('The Environment gives a -1 Reward for each Step in order for the Agent to try and find the quickest solution.')\n",
    "print('The Environment gives a -10 Reward if Agent incorrectly Picks Up or Drops Off a Passenger.')\n",
    "print('The Environment gives a 20 Reward on Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Action Size ', 6)\n",
      "('State Size ', 500)\n"
     ]
    }
   ],
   "source": [
    "# LOAD ENVIRONMENT\n",
    "# Explore Environment\n",
    "ActionSize=Env.action_space.n\n",
    "print(\"Action Size \",ActionSize)\n",
    "StateSize=Env.observation_space.n\n",
    "print(\"State Size \",StateSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZATION\n",
    "# Initialize Q-Table\n",
    "QTable=np.zeros((StateSize,ActionSize))\n",
    "print(QTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add Hyper-Parameters\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZATION\n",
    "# Add Hyper-Parameters Episodes\n",
    "TotalEpisodes=50000        # Total Episodes\n",
    "TotalTestEpisodes=1        # Total Test Episodes\n",
    "MaxSteps=99                # Max Steps per Episode\n",
    "\n",
    "# Add Hyper-Parameters Bellman Equation\n",
    "LearningRate=0.7           # Learning Rate\n",
    "Gamma=0.618                # Discounting Rate\n",
    "\n",
    "# Add Exploration Parameters\n",
    "Epsilon=1.0                # Exploration rate\n",
    "MaxEpsilon=1.0             # Exploration probability at start\n",
    "MinEpsilon=0.01            # Minimum exploration probability \n",
    "DecayRate=0.01             # Exponential decay rate for exploration prob\n",
    "print('Add Hyper-Parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -1.89494435  -1.44813002  -1.89494435  -1.44813002  -0.72512948\n",
      "  -10.44813001]\n",
      " [ -0.72512972   0.4447743   -0.72516003   0.44477417   2.33782249\n",
      "   -8.55522572]\n",
      " ...\n",
      " [  1.04882142   5.40100727   1.01684373  -2.11715326 -10.50803626\n",
      "   -6.68950693]\n",
      " [ -1.94725029  -1.54576149  -2.47610015  -1.44813002 -10.88646532\n",
      "   -7.        ]\n",
      " [ 12.65461466  -0.7         16.66099906  31.35602094   0.\n",
      "    6.56461466]]\n"
     ]
    }
   ],
   "source": [
    "# Q-LEARNING\n",
    "# Perform Learning for each Episode\n",
    "for Episode in range(TotalEpisodes):\n",
    "    # Reset the Environment\n",
    "    State=Env.reset()\n",
    "    Step=0\n",
    "    Done=False\n",
    "    \n",
    "    # Perform Temporal Difference Learning for each Step\n",
    "    for Step in range(MaxSteps):\n",
    "        # Choose an Action (A) in Current World State (S)\n",
    "        # First Randomize a Number\n",
    "        ExploreExploitTradeoff=random.uniform(0,1)\n",
    "        \n",
    "        # Check if this number is Greater than Epsilon, Then Exploitation (Take the Biggest Q-Value for this State)\n",
    "        if ExploreExploitTradeoff > Epsilon:\n",
    "            Action=np.argmax(QTable[State,:])\n",
    "        \n",
    "        # Otherwise Exploration (Perform a Random Action)\n",
    "        else:\n",
    "            Action=Env.action_space.sample()\n",
    "        \n",
    "        # Take the Action (A) and Observe the Outcome State(S') and Reward (R)\n",
    "        NewState,Reward,Done,Info=Env.step(Action)\n",
    "\n",
    "        # Update Q(S,A):= Q(S,A) + Learning Rate * [R(S,A) + Gamma * Max Q(S',A') - Q(S,A)]\n",
    "        QTable[State,Action]=QTable[State,Action]+LearningRate*(Reward+Gamma*np.max(QTable[NewState,:])-QTable[State,Action])\n",
    "                \n",
    "        # Update State\n",
    "        State=NewState\n",
    "        \n",
    "        # Check if Episode is Finished\n",
    "        if Done==True: \n",
    "            break\n",
    "            \n",
    "    # Increment Episode    \n",
    "    Episode+=1\n",
    "    \n",
    "    # Reduce Epsilon (We Need Less and Less Exploration after each Episode)\n",
    "    Epsilon=MinEpsilon+(MaxEpsilon-MinEpsilon)*np.exp(-DecayRate*Episode)\n",
    "\n",
    "# Print Final Q-Table\n",
    "print(QTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('Episode ', 0)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[42mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | :\u001b[42m_\u001b[0m:G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: |\u001b[42m_\u001b[0m: :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "('Score ', 6)\n",
      "Score Over Time: 6\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "# Test the Q-Learning via Playing\n",
    "Env.reset()\n",
    "Rewards=[]\n",
    "\n",
    "# Run Player for each Episode\n",
    "for Episode in range(TotalTestEpisodes):\n",
    "    State=Env.reset()\n",
    "    Step=0\n",
    "    Done=False\n",
    "    TotalRewards=0\n",
    "    print(\"\")\n",
    "    print(\"Episode \",Episode)\n",
    "    for Step in range(MaxSteps):\n",
    "        Env.render()\n",
    "        # Take the Action (A) that have the Maximum Expected Future Reward given that State\n",
    "        Action=np.argmax(QTable[State,:])\n",
    "        \n",
    "        # Update State\n",
    "        NewState,Reward,Done,Info=Env.step(Action)\n",
    "        \n",
    "        # Update Rewards\n",
    "        TotalRewards +=Reward\n",
    "        \n",
    "        # Check if Task Completed\n",
    "        if Done:\n",
    "            Rewards.append(TotalRewards)\n",
    "            print (\"Score \",TotalRewards)\n",
    "            break\n",
    "        \n",
    "        # Update State\n",
    "        State=NewState\n",
    "        \n",
    "# Close Environment        \n",
    "Env.close()\n",
    "print (\"Score Over Time: \"+str(sum(Rewards)/TotalTestEpisodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
